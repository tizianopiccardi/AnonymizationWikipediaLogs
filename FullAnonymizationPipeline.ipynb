{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import timedelta, date\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.functions as psf\n",
    "from urllib.parse import unquote, urlparse, parse_qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESTINATION_FOLDER = \"how_we_read_wikipedia_march_en\"\n",
    "\n",
    "start_date = date(2021, 2, 28)\n",
    "end_date = date(2021, 4, 2)\n",
    "SNAPSHOT = '2021-03'\n",
    "\n",
    "# required to iterate the range of dates\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random salt applied both to IP and user-agent\n",
    "\n",
    "The salt strings are never printed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "def get_random_string(size):\n",
    "    return ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(size))\n",
    "\n",
    "salt1 = get_random_string(16)\n",
    "salt2 = get_random_string(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of anonymized user identifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of user identifier (different salt):\n",
      "\n",
      "md5(<IP>4U2KHO9AYXULSIGB<ACCESS_METHOD>) + md5(<USER_AGENT>8O9UKZVPFRBSP7AY) = \n",
      "\te147ad5df076b246b60d4fc7c98a2f3a6e135561ff80a4bbc8b019c52d52766d\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "# Different salt. The secret one is never printed\n",
    "s1 = get_random_string(16)\n",
    "s2 = get_random_string(16)\n",
    "print(\"Example of user identifier (different salt):\\n\")\n",
    "print(\"md5(<IP>{}<ACCESS_METHOD>) + md5(<USER_AGENT>{}) = \".format(s1, s2))\n",
    "print(\"\\t\"+hashlib.md5(\"<IP>{}<ACCESS_METHOD>\".format(s1).encode()).hexdigest()+\n",
    "      hashlib.md5(\"<USER_AGENT>{}\".format(s2).encode()).hexdigest())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get countries with more that 500 views every day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    select geocoded_data['country_code'] country_code\n",
    "    from wmf.webrequest \n",
    "    where day = {}\n",
    "    AND month = {}\n",
    "    AND year = {}\n",
    "    AND x_analytics_map['loggedIn'] is NULL\n",
    "    AND (namespace_id = 0 OR namespace_id = 1 OR namespace_id = 4 OR namespace_id = 100)\n",
    "    AND agent_type = 'user'\n",
    "    AND is_pageview = TRUE\n",
    "    AND (uri_host = 'en.wikipedia.org' or uri_host = 'en.m.wikipedia.org')\n",
    "    AND http_method= 'GET'\n",
    "    AND ts is not NULL\n",
    "\"\"\"\n",
    "\n",
    "countries = set()\n",
    "for day in daterange(start_date, end_date):\n",
    "    # Load the logs of the day\n",
    "    pageview_logs = spark.sql(query.format(day.day, day.month, day.year)).repartition(300)\n",
    "    # Group by country\n",
    "    pageviews_by_country = pageview_logs.groupBy(\"country_code\").agg(count(\"*\").alias(\"total\"))\n",
    "    # Keep only countries with 500 views in this day\n",
    "    pageviews_by_country_filtered = pageviews_by_country.where(\"total>=500\")\n",
    "    # Collect the countries names\n",
    "    countries500views = pageviews_by_country_filtered.select(\"country_code\").collect()\n",
    "    # Get the intersection with the other days\n",
    "    if len(countries) < 1:\n",
    "        countries = set(countries500views)\n",
    "    else:\n",
    "        countries = countries.intersection(countries500views)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of the countries included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--', 'AD', 'AE', 'AF', 'AG', 'AI', 'AL', 'AM', 'AO', 'AR', 'AS', 'AT', 'AU', 'AW', 'AX', 'AZ', 'BA', 'BB', 'BD', 'BE', 'BF', 'BG', 'BH', 'BI', 'BJ', 'BM', 'BN', 'BO', 'BQ', 'BR', 'BS', 'BT', 'BW', 'BY', 'BZ', 'CA', 'CD', 'CH', 'CI', 'CK', 'CL', 'CM', 'CN', 'CO', 'CR', 'CU', 'CV', 'CW', 'CY', 'CZ', 'DE', 'DJ', 'DK', 'DM', 'DO', 'DZ', 'EC', 'EE', 'EG', 'ER', 'ES', 'ET', 'FI', 'FJ', 'FK', 'FM', 'FO', 'FR', 'GA', 'GB', 'GD', 'GE', 'GF', 'GG', 'GH', 'GI', 'GL', 'GM', 'GN', 'GP', 'GQ', 'GR', 'GT', 'GU', 'GY', 'HK', 'HN', 'HR', 'HT', 'HU', 'ID', 'IE', 'IL', 'IM', 'IN', 'IQ', 'IR', 'IS', 'IT', 'JE', 'JM', 'JO', 'JP', 'KE', 'KG', 'KH', 'KN', 'KR', 'KW', 'KY', 'KZ', 'LA', 'LB', 'LC', 'LI', 'LK', 'LR', 'LS', 'LT', 'LU', 'LV', 'LY', 'MA', 'MC', 'MD', 'ME', 'MF', 'MG', 'MH', 'MK', 'ML', 'MM', 'MN', 'MO', 'MP', 'MQ', 'MR', 'MT', 'MU', 'MV', 'MW', 'MX', 'MY', 'MZ', 'NA', 'NC', 'NE', 'NG', 'NI', 'NL', 'NO', 'NP', 'NZ', 'OM', 'PA', 'PE', 'PF', 'PG', 'PH', 'PK', 'PL', 'PR', 'PS', 'PT', 'PW', 'PY', 'QA', 'RE', 'RO', 'RS', 'RU', 'RW', 'SA', 'SB', 'SC', 'SD', 'SE', 'SG', 'SI', 'SK', 'SL', 'SM', 'SN', 'SO', 'SR', 'SS', 'SV', 'SX', 'SY', 'SZ', 'TC', 'TG', 'TH', 'TJ', 'TL', 'TM', 'TN', 'TO', 'TR', 'TT', 'TW', 'TZ', 'UA', 'UG', 'US', 'UY', 'UZ', 'VC', 'VE', 'VG', 'VI', 'VN', 'VU', 'WS', 'XK', 'YE', 'ZA', 'ZM', 'ZW']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sorted([c.country_code for c in countries]))\n",
    "len(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries_whitelist.write.parquet(\"countries_whitelist.parquet\")\n",
    "# countries_whitelist = spark.read.parquet(\"countries_whitelist.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|country_code|\n",
      "+------------+\n",
      "|          SO|\n",
      "|          AR|\n",
      "|          TG|\n",
      "|          SI|\n",
      "|          PR|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_whitelist = spark.createDataFrame(sc.parallelize(list(countries))).cache()\n",
    "countries_whitelist.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the users with edits\n",
    "\n",
    "Get all the users indentifiers with an edit. Get the `user_identifier` and the day/month localised with the local timezone. This rows will left joined later as a blacklist to exclude all views of that user in the day of the edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total editors-day: 8310025\n",
      "Total editors: 7447391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[user_identifier: string, day: bigint, month: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select CONCAT(MD5(CONCAT(CONCAT(client_ip, '{}'), access_method)), MD5(CONCAT(user_agent, '{}'))) as user_identifier,\n",
    "        from_utc_timestamp(ts, geocoded_data['timezone']) as local_time\n",
    "from wmf.webrequest \n",
    "where day = {}\n",
    "AND month = {}\n",
    "AND year = {}\n",
    "AND agent_type = 'user'\n",
    "AND http_method= 'GET'\n",
    "AND ts is not NULL\n",
    "AND (uri_host = 'en.wikipedia.org' or uri_host = 'en.m.wikipedia.org')\n",
    "AND (uri_query LIKE '%action=edit%' OR uri_query LIKE '%action=visualeditor%' \n",
    "    OR uri_query LIKE '%&intestactions=edit&intestactionsdetail=full&uiprop=options%')\n",
    "\"\"\"\n",
    "\n",
    "edits_rdd = sc.emptyRDD()\n",
    "for day in daterange(start_date, end_date):\n",
    "    daily_edits_rdd = spark.sql(query.format(salt1, salt2, day.day, day.month, day.year))\\\n",
    "                .selectExpr(\"user_identifier\", \"day(local_time) day\", \"month(local_time) month\")\\\n",
    "                .distinct().repartition(50).rdd\n",
    "    edits_rdd = edits_rdd.union(daily_edits_rdd)\n",
    "    \n",
    "editors = spark.createDataFrame(edits_rdd).distinct().repartition(200).cache()\n",
    "\n",
    "print(\"Total editors-day:\", editors.count())\n",
    "print(\"Total editors:\", editors.select(\"user_identifier\").distinct().count())\n",
    "editors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "### Function to drop sensitive information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "editors.registerTempTable(\"editors\")\n",
    "\n",
    "def without_sensitive_data(current_dataframe):\n",
    "    # Add the day and month used in the join with the editors\n",
    "    current_dataframe = current_dataframe.selectExpr(\"*\", \"day(local_time) day\", \"month(local_time) month\")\n",
    "    current_dataframe.createOrReplaceTempView(\"current_dataframe\")\n",
    "    # LEFT JOIN on identifier and day/month. Keep non-matches to exclude editors\n",
    "    query_no_editors = \"\"\"\n",
    "        SELECT c.*\n",
    "        FROM current_dataframe c\n",
    "        LEFT JOIN editors e\n",
    "        ON c.user_identifier = e.user_identifier\n",
    "        AND c.day = e.day\n",
    "        AND c.month = e.month\n",
    "        WHERE e.user_identifier IS NULL\n",
    "    \"\"\"\n",
    "    clean_dataframe = spark.sql(query_no_editors).drop(\"day\").drop(\"month\")\n",
    "    # Join the dataframe with the whitelisted countries\n",
    "    return clean_dataframe.join(countries_whitelist, \"country_code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "# Get page views\n",
    "\n",
    "Let's start with the redirects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageid_title = spark.sql(\"\"\"\n",
    "    SELECT page_id, page_title\n",
    "    FROM wmf_raw.mediawiki_page\n",
    "    WHERE wiki_db = 'enwiki'\n",
    "    AND snapshot = '{}'\n",
    "    AND (page_namespace = 0 OR page_namespace = 4 OR page_namespace = 100 OR page_namespace = 1)\n",
    "\"\"\".format(SNAPSHOT)).cache()\n",
    "\n",
    "pageid_title.registerTempTable(\"pageid_title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the pageviews. There is not need to anonymize the IP in this stage. We will drop it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-28 Country & editors filter keeps 97.11862413358534%\n",
      "2021-02-28 DONE\n",
      "2021-03-01 Country & editors filter keeps 96.47104325972714%\n",
      "2021-03-01 DONE\n",
      "2021-03-02 Country & editors filter keeps 96.66165474111801%\n",
      "2021-03-02 DONE\n",
      "2021-03-03 Country & editors filter keeps 96.67920103590905%\n",
      "2021-03-03 DONE\n",
      "2021-03-04 Country & editors filter keeps 96.62495644357756%\n",
      "2021-03-04 DONE\n",
      "2021-03-05 Country & editors filter keeps 96.9129059311342%\n",
      "2021-03-05 DONE\n",
      "2021-03-06 Country & editors filter keeps 97.51811161478103%\n",
      "2021-03-06 DONE\n",
      "2021-03-07 Country & editors filter keeps 97.53978752320066%\n",
      "2021-03-07 DONE\n",
      "2021-03-08 Country & editors filter keeps 96.95045206695126%\n",
      "2021-03-08 DONE\n",
      "2021-03-09 Country & editors filter keeps 96.61660340743691%\n",
      "2021-03-09 DONE\n",
      "2021-03-10 Country & editors filter keeps 96.80954208338704%\n",
      "2021-03-10 DONE\n",
      "2021-03-11 Country & editors filter keeps 96.40275064787714%\n",
      "2021-03-11 DONE\n",
      "2021-03-12 Country & editors filter keeps 96.8221734280131%\n",
      "2021-03-12 DONE\n",
      "2021-03-13 Country & editors filter keeps 97.4970078409982%\n",
      "2021-03-13 DONE\n",
      "2021-03-14 Country & editors filter keeps 97.29734446015348%\n",
      "2021-03-14 DONE\n",
      "2021-03-15 Country & editors filter keeps 96.87578187504326%\n",
      "2021-03-15 DONE\n",
      "2021-03-16 Country & editors filter keeps 96.87718974221481%\n",
      "2021-03-16 DONE\n",
      "2021-03-17 Country & editors filter keeps 97.02861579623915%\n",
      "2021-03-17 DONE\n",
      "2021-03-18 Country & editors filter keeps 96.87698050477458%\n",
      "2021-03-18 DONE\n",
      "2021-03-19 Country & editors filter keeps 96.92785244959737%\n",
      "2021-03-19 DONE\n",
      "2021-03-20 Country & editors filter keeps 97.2002221113843%\n",
      "2021-03-20 DONE\n",
      "2021-03-21 Country & editors filter keeps 97.42372756285252%\n",
      "2021-03-21 DONE\n",
      "2021-03-22 Country & editors filter keeps 96.89503968158526%\n",
      "2021-03-22 DONE\n",
      "2021-03-23 Country & editors filter keeps 96.33769507100824%\n",
      "2021-03-23 DONE\n",
      "2021-03-24 Country & editors filter keeps 96.56610629624245%\n",
      "2021-03-24 DONE\n",
      "2021-03-25 Country & editors filter keeps 97.00120240793804%\n",
      "2021-03-25 DONE\n",
      "2021-03-26 Country & editors filter keeps 96.7932889179514%\n",
      "2021-03-26 DONE\n",
      "2021-03-27 Country & editors filter keeps 97.04461556361314%\n",
      "2021-03-27 DONE\n",
      "2021-03-28 Country & editors filter keeps 97.28915168921789%\n",
      "2021-03-28 DONE\n",
      "2021-03-29 Country & editors filter keeps 96.5526054834757%\n",
      "2021-03-29 DONE\n",
      "2021-03-30 Country & editors filter keeps 96.61463040904053%\n",
      "2021-03-30 DONE\n",
      "2021-03-31 Country & editors filter keeps 96.41778810670625%\n",
      "2021-03-31 DONE\n",
      "2021-04-01 Country & editors filter keeps 96.59195937136887%\n",
      "2021-04-01 DONE\n",
      "[97.11862413358534, 96.47104325972714, 96.66165474111801, 96.67920103590905, 96.62495644357756, 96.9129059311342, 97.51811161478103, 97.53978752320066, 96.95045206695126, 96.61660340743691, 96.80954208338704, 96.40275064787714, 96.8221734280131, 97.4970078409982, 97.29734446015348, 96.87578187504326, 96.87718974221481, 97.02861579623915, 96.87698050477458, 96.92785244959737, 97.2002221113843, 97.42372756285252, 96.89503968158526, 96.33769507100824, 96.56610629624245, 97.00120240793804, 96.7932889179514, 97.04461556361314, 97.28915168921789, 96.5526054834757, 96.61463040904053, 96.41778810670625, 96.59195937136887]\n",
      "Format:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[country_code: string, user_identifier: string, http_status: string, referer: string, local_time: timestamp, timezone: string, access_method: string, page_title: string, page_id: bigint, namespace_id: int, actual_destination: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# User must be not loggedIn, not a bot and the view must be in the English version of Wikipedia\n",
    "query = \"\"\"\n",
    "select CONCAT(MD5(CONCAT(CONCAT(client_ip, '{}'), access_method)), MD5(CONCAT(user_agent, '{}'))) as user_identifier,\n",
    "        geocoded_data['country_code'] country_code,\n",
    "        http_status, referer,\n",
    "        from_utc_timestamp(ts, geocoded_data['timezone']) as local_time, \n",
    "        geocoded_data['timezone'] timezone, access_method,\n",
    "        pageview_info['page_title'] as page_title, page_id, namespace_id\n",
    "from wmf.webrequest \n",
    "where day = {}\n",
    "AND month = {}\n",
    "AND year = {}\n",
    "AND x_analytics_map['loggedIn'] is NULL\n",
    "AND (namespace_id = 0 OR namespace_id = 4 OR namespace_id = 100 OR namespace_id = 1)\n",
    "AND agent_type = 'user'\n",
    "AND is_pageview = TRUE\n",
    "AND (uri_host = 'en.wikipedia.org' or uri_host = 'en.m.wikipedia.org')\n",
    "AND http_method= 'GET'\n",
    "AND ts is not NULL\n",
    "AND page_id is not NULL\n",
    "\"\"\"\n",
    "\n",
    "kept_ratios = []\n",
    "\n",
    "# Repeat for each day\n",
    "for day in daterange(start_date, end_date):\n",
    "    pageview_logs = spark.sql(query.format(salt1, salt2, day.day, day.month, day.year)).repartition(50)\n",
    "    pageview_logs.createOrReplaceTempView(\"pageview_logs\")\n",
    "    \n",
    "#     # Add resolved redirects (add the day and month for later join)\n",
    "    query_join_redirects = \"\"\"\n",
    "        SELECT l.*, \n",
    "            CASE WHEN pid.page_title is NULL \n",
    "                THEN l.page_title \n",
    "                ELSE pid.page_title \n",
    "            END as actual_destination\n",
    "        FROM pageview_logs l\n",
    "        LEFT JOIN pageid_title pid\n",
    "        ON l.page_id = pid.page_id\n",
    "    \"\"\"\n",
    "    logs_redirect_resolved = spark.sql(query_join_redirects).cache()\n",
    "    \n",
    "    # Remove sensitive data\n",
    "    logs_filtered = without_sensitive_data(logs_redirect_resolved).cache()\n",
    "\n",
    "    kept_proportion = 100 * logs_filtered.count() / logs_redirect_resolved.count()\n",
    "    kept_ratios.append(kept_proportion)\n",
    "    print(\"{}-{:02d}-{:02d} Country & editors filter keeps {}%\".format(day.year, day.month, day.day, \n",
    "                                     kept_proportion))\n",
    "    # Write the pageviews of this day\n",
    "    logs_filtered.write.mode(\"overwrite\")\\\n",
    "                .parquet(\"{}/pageloads/{}{:02d}{:02d}\".format(DESTINATION_FOLDER, day.year, day.month, day.day))\n",
    "    print(\"{}-{:02d}-{:02d} DONE\".format(day.year, day.month, day.day))\n",
    "    \n",
    "    # Drop cached dataframes\n",
    "    logs_redirect_resolved.unpersist()\n",
    "    logs_filtered.unpersist()\n",
    "\n",
    "print(kept_ratios)\n",
    "print(\"Format:\")\n",
    "logs_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get page previews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-28 DONE\n",
      "2021-03-01 DONE\n",
      "2021-03-02 DONE\n",
      "2021-03-03 DONE\n",
      "2021-03-04 DONE\n",
      "2021-03-05 DONE\n",
      "2021-03-06 DONE\n",
      "2021-03-07 DONE\n",
      "2021-03-08 DONE\n",
      "2021-03-09 DONE\n",
      "2021-03-10 DONE\n",
      "2021-03-11 DONE\n",
      "2021-03-12 DONE\n",
      "2021-03-13 DONE\n",
      "2021-03-14 DONE\n",
      "2021-03-15 DONE\n",
      "2021-03-16 DONE\n",
      "2021-03-17 DONE\n",
      "2021-03-18 DONE\n",
      "2021-03-19 DONE\n",
      "2021-03-20 DONE\n",
      "2021-03-21 DONE\n",
      "2021-03-22 DONE\n",
      "2021-03-23 DONE\n",
      "2021-03-24 DONE\n",
      "2021-03-25 DONE\n",
      "2021-03-26 DONE\n",
      "2021-03-27 DONE\n",
      "2021-03-28 DONE\n",
      "2021-03-29 DONE\n",
      "2021-03-30 DONE\n",
      "2021-03-31 DONE\n",
      "2021-04-01 DONE\n",
      "Format:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[user_identifier: string, http_status: string, local_time: timestamp, preview_title: string, page_title: string, long_preview: boolean]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_page_udf = udf(lambda uri_path: unquote(uri_path[uri_path.rfind('/')+1:]), StringType())\n",
    "\n",
    "# Consider all previews. Note:\n",
    "#    year year_original, month month_original, day day_original, \n",
    "#    hour hour_original, client_ip, user_agent_map: are required for the join with the long previews\n",
    "# We'll discard them later.\n",
    "preview_query = \"\"\"\n",
    "select CONCAT(MD5(CONCAT(CONCAT(client_ip, '{}'), access_method)), MD5(CONCAT(user_agent, '{}'))) as user_identifier,\n",
    "        http_status, referer,\n",
    "        from_utc_timestamp(ts, geocoded_data['timezone']) as local_time,\n",
    "        uri_path,\n",
    "        geocoded_data['country_code'] country_code,\n",
    "        \n",
    "        year year_original, month month_original, day day_original, hour hour_original, client_ip, user_agent_map\n",
    "from wmf.webrequest \n",
    "where day = {}\n",
    "AND month = {}\n",
    "AND year = {}\n",
    "AND x_analytics_map['loggedIn'] is NULL\n",
    "AND agent_type = 'user'\n",
    "AND http_method= 'GET'\n",
    "AND ts is not NULL\n",
    "AND uri_host = 'en.wikipedia.org'\n",
    "AND uri_path LIKE '/api/rest_v1/page/summary/%'\n",
    "AND referer LIKE '%wikipedia.org/wiki%'\n",
    "\"\"\"\n",
    "\n",
    "# Consider the previews with at least 1 second\n",
    "long_previews_query = \"\"\"\n",
    "select ip, \n",
    "    year, month, day, hour,\n",
    "    useragent.browser_family,\n",
    "    useragent.browser_major,\n",
    "    useragent.os_family,\n",
    "    useragent.device_family,\n",
    "    event.source_title, event.page_title preview_title\n",
    "from event.virtualpageview\n",
    "where year = {}\n",
    "and month = {}\n",
    "and day = {}\n",
    "and webhost = 'en.wikipedia.org'\n",
    "and useragent.is_bot = false\n",
    "\"\"\"\n",
    "\n",
    "for day in daterange(start_date, end_date):\n",
    "    # compile the query with parameters and repartition the data\n",
    "    logs = spark.sql(preview_query.format(salt1, salt2, day.day, day.month, day.year)).repartition(30)\n",
    "    \n",
    "    # Extract the titles from the urls and drop the original fields\n",
    "    logs_cleaned = logs.withColumn(\"preview_title\", get_page_udf(logs.uri_path)).drop(\"uri_path\")\\\n",
    "                    .withColumn(\"page_title\", get_page_udf(logs.referer)).drop(\"referer\")\n",
    "    \n",
    "    # Remove sensitive data\n",
    "    previews_cleaned = without_sensitive_data(logs_cleaned).drop(\"country_code\")\n",
    "    \n",
    "    # Get long previews\n",
    "    long_previews = spark.sql(long_previews_query.format(day.year, day.month, day.day)).distinct()\n",
    "\n",
    "    # Register the tables\n",
    "    previews_cleaned.createOrReplaceTempView(\"previews_cleaned\")\n",
    "    long_previews.createOrReplaceTempView(\"long_previews\")\n",
    "\n",
    "    # LEFT JOIN previews_cleaned with the long previews to add a label TRUE or FALSE\n",
    "    # Since the 2 tables have different schema, we have to maximise the matching by joining\n",
    "    # on all the fields we can use. We add a boolean labels to mark if we found a long preview\n",
    "    # from the same ip, on the same page, for the same link, in the the same hour, \n",
    "    # with the same browser/OS version\n",
    "    query = \"\"\"\n",
    "        SELECT ap.*,\n",
    "            CASE \n",
    "                WHEN lp.ip IS NULL THEN FALSE\n",
    "                ELSE TRUE\n",
    "            END as long_preview\n",
    "        FROM previews_cleaned ap\n",
    "        LEFT JOIN long_previews lp\n",
    "        ON ap.client_ip = lp.ip\n",
    "        AND ap.hour_original = lp.hour\n",
    "        AND ap.day_original = lp.day\n",
    "        AND ap.month_original = lp.month\n",
    "        AND ap.year_original = lp.year\n",
    "        AND ap.page_title = lp.source_title\n",
    "        AND ap.preview_title = lp.preview_title\n",
    "        AND ap.user_agent_map.browser_family = lp.browser_family\n",
    "        AND ap.user_agent_map.browser_major = lp.browser_major\n",
    "        AND ap.user_agent_map.os_family = lp.os_family\n",
    "        AND ap.user_agent_map.device_family = lp.device_family\n",
    "    \"\"\"\n",
    "\n",
    "    all_previews_labeled = spark.sql(query).drop(\"user_agent_map\", \"client_ip\", \"year_original\", \n",
    "                                                 \"month_original\", \"day_original\", \"hour_original\")\n",
    "    \n",
    "    # Write the previews\n",
    "    all_previews_labeled.write.mode(\"overwrite\")\\\n",
    "                .parquet(\"{}/previews/{}{:02d}{:02d}\".format(DESTINATION_FOLDER, day.year, day.month, day.day))\n",
    "    print(\"{}-{:02d}-{:02d} DONE\".format(day.year, day.month, day.day))\n",
    "    \n",
    "print(\"Format:\")\n",
    "all_previews_labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Get clicks on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a row with the image name and title extracted from the urls\n",
    "# Input format:\n",
    "# DataFrame[user_identifier: string, local_time: timestamp, \n",
    "#           referer: string, uri_query: string, access_method: string]\n",
    "\n",
    "import re\n",
    "from urllib.parse import unquote, urlparse\n",
    "\n",
    "thumb_regex = re.compile(r\"upload\\.wikimedia\\.org/wikipedia/commons/thumb/\\w/\\w+/([^/]+)/.*\")\n",
    "\n",
    "def get_page_title(url):\n",
    "    \"\"\"Extract the page_title from the referer.\"\"\"\n",
    "    try:\n",
    "        url_decoded = unquote(url)\n",
    "        url_path = urlparse(url_decoded).path\n",
    "        page_title = url_path.split('/wiki/')[1]\n",
    "        return page_title\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_formatted_row(row):\n",
    "    try:\n",
    "        # Take the string after the last / and convert the http encoded text\n",
    "        title = get_page_title(row.referer) #unquote(row.referer[row.referer.rfind('/')+1:])\n",
    "        # Parse the query string\n",
    "        parsed_query = parse_qs(unquote(row.uri_query))\n",
    "        # image name\n",
    "        image_name=None\n",
    "        # Desktop and mobile have different formats\n",
    "        if row.access_method == 'desktop':\n",
    "            # Get the parameters called uri\n",
    "            image_url = parsed_query.get(\"uri\")\n",
    "            if len(image_url)>0:\n",
    "                if 'commons/thumb' not in image_url[0]:\n",
    "                    image_name = unquote(image_url[0][image_url[0].rfind('/')+1:])\n",
    "                else:\n",
    "                    image_name_search = thumb_regex.search(image_url[0])\n",
    "                    if image_name_search:\n",
    "                        image_name = unquote(image_name_search.group(1))\n",
    "        else:\n",
    "            # Get the titles parameter and then the string after File:\n",
    "            file_name = parsed_query.get('titles')[0]\n",
    "            image_title = file_name.split('File:')[1]\n",
    "            image_name = unquote(image_title)\n",
    "    except:\n",
    "        # If there is no image return null\n",
    "        image_name = None\n",
    "    \n",
    "    return Row(user_identifier=row.user_identifier, access_method=row.access_method, local_time=row.local_time,\n",
    "                       page_title=title, image_name=image_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-28 DONE\n",
      "2021-03-01 DONE\n",
      "2021-03-02 DONE\n",
      "2021-03-03 DONE\n",
      "2021-03-04 DONE\n",
      "2021-03-05 DONE\n",
      "2021-03-06 DONE\n",
      "2021-03-07 DONE\n",
      "2021-03-08 DONE\n",
      "2021-03-09 DONE\n",
      "2021-03-10 DONE\n",
      "2021-03-11 DONE\n",
      "2021-03-12 DONE\n",
      "2021-03-13 DONE\n",
      "2021-03-14 DONE\n",
      "2021-03-15 DONE\n",
      "2021-03-16 DONE\n",
      "2021-03-17 DONE\n",
      "2021-03-18 DONE\n",
      "2021-03-19 DONE\n",
      "2021-03-20 DONE\n",
      "2021-03-21 DONE\n",
      "2021-03-22 DONE\n",
      "2021-03-23 DONE\n",
      "2021-03-24 DONE\n",
      "2021-03-25 DONE\n",
      "2021-03-26 DONE\n",
      "2021-03-27 DONE\n",
      "2021-03-28 DONE\n",
      "2021-03-29 DONE\n",
      "2021-03-30 DONE\n",
      "2021-03-31 DONE\n",
      "2021-04-01 DONE\n",
      "Format:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[access_method: string, image_name: string, local_time: timestamp, page_title: string, user_identifier: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The referer filter limits to image previews opened from Wikipedia articles\n",
    "imageviews_query = \"\"\"\n",
    "    select CONCAT(MD5(CONCAT(CONCAT(client_ip, '{}'), access_method)), MD5(CONCAT(user_agent, '{}'))) as user_identifier,\n",
    "        from_utc_timestamp(ts, geocoded_data['timezone']) as local_time,\n",
    "        referer, uri_query, access_method,\n",
    "        geocoded_data['country_code'] country_code\n",
    "    from wmf.webrequest \n",
    "    where day = {}\n",
    "    AND month = {}\n",
    "    AND year = {}\n",
    "    AND x_analytics_map['loggedIn'] is NULL\n",
    "    AND agent_type = 'user'\n",
    "    AND ts is not NULL\n",
    "    AND (uri_host = 'en.wikipedia.org' or uri_host = 'en.m.wikipedia.org')\n",
    "    AND referer LIKE '%wikipedia.org%'\n",
    "    AND (\n",
    "           (uri_path LIKE '%/beacon/media%' AND access_method = 'desktop')\n",
    "        OR (uri_path = '/w/api.php' AND uri_query LIKE '%prop=imageinfo%' AND access_method = 'mobile web')\n",
    "        )\n",
    "\"\"\"\n",
    "\n",
    "for day in daterange(start_date, end_date):\n",
    "    imageviews = spark.sql(imageviews_query.format(salt1, salt2, day.day, day.month, day.year)).repartition(30)\n",
    "    \n",
    "    # Remove sensitive data\n",
    "    imageviews_cleaned = without_sensitive_data(imageviews).drop(\"country_code\")\n",
    "    \n",
    "    # Extract the titles from the urls and drop the original fields\n",
    "    imageviews_with_titles = spark.createDataFrame(imageviews_cleaned.rdd.map(get_formatted_row))\n",
    "    imageviews_with_titles.write.mode(\"overwrite\")\\\n",
    "                .parquet(\"{}/imageviews/{}{:02d}{:02d}\".format(DESTINATION_FOLDER, day.year, day.month, day.day))\n",
    "    print(\"{}-{:02d}-{:02d} DONE\".format(day.year, day.month, day.day))\n",
    "    \n",
    "print(\"Format:\")\n",
    "imageviews_with_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -ls how_we_read_wikipedia_march_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark - YARN (large)",
   "language": "python",
   "name": "spark_yarn_pyspark_large"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
