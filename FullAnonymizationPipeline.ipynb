{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import timedelta, date\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.functions as psf\n",
    "from urllib.parse import unquote, urlparse, parse_qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESTINATION_FOLDER = \"how_we_read_wikipedia\"\n",
    "\n",
    "start_date = date(2021, 1, 3)\n",
    "end_date = date(2021, 2, 1)\n",
    "SNAPSHOT = '2021-01'\n",
    "\n",
    "# required to iterate the range of dates\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random salt applied both to IP and user-agent\n",
    "\n",
    "The salt strings are never printed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "def get_random_string(size):\n",
    "    return ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(size))\n",
    "\n",
    "salt1 = get_random_string(16)\n",
    "salt2 = get_random_string(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of anonymized user identifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of user identifier (different salt):\n",
      "\n",
      "md5(<IP>BIZ4QS0VQBO75NKM<ACCESS_METHOD>) + md5(<USER_AGENT>E8N2XB146ZHO9YOQ) = \n",
      "\t4e5e6498f29a8f34a8bbb88021a3d5458332b574bf83332111aec3f20146a31b\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "# Different salt. The secret one is never printed\n",
    "s1 = get_random_string(16)\n",
    "s2 = get_random_string(16)\n",
    "print(\"Example of user identifier (different salt):\\n\")\n",
    "print(\"md5(<IP>{}<ACCESS_METHOD>) + md5(<USER_AGENT>{}) = \".format(s1, s2))\n",
    "print(\"\\t\"+hashlib.md5(\"<IP>{}<ACCESS_METHOD>\".format(s1).encode()).hexdigest()+\n",
    "      hashlib.md5(\"<USER_AGENT>{}\".format(s2).encode()).hexdigest())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get countries with more that 500 views every day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    select geocoded_data['country_code'] country_code\n",
    "    from wmf.webrequest \n",
    "    where day = {}\n",
    "    AND month = {}\n",
    "    AND year = {}\n",
    "    AND x_analytics_map['loggedIn'] is NULL\n",
    "    AND (namespace_id = 0 OR namespace_id = 4 OR namespace_id = 100)\n",
    "    AND agent_type = 'user'\n",
    "    AND is_pageview = TRUE\n",
    "    AND (uri_host = 'en.wikipedia.org' or uri_host = 'en.m.wikipedia.org')\n",
    "    AND http_method= 'GET'\n",
    "    AND ts is not NULL\n",
    "\"\"\"\n",
    "\n",
    "countries = set()\n",
    "for day in daterange(start_date, end_date):\n",
    "    # Load the logs of the day\n",
    "    pageview_logs = spark.sql(query.format(day.day, day.month, day.year)).repartition(30)\n",
    "    # Group by country\n",
    "    pageviews_by_country = pageview_logs.groupBy(\"country_code\").agg(count(\"*\").alias(\"total\"))\n",
    "    # Keep only countries with 500 views in this day\n",
    "    pageviews_by_country_filtered = pageviews_by_country.where(\"total>=500\")\n",
    "    # Collect the countries names\n",
    "    countries500views = pageviews_by_country_filtered.select(\"country_code\").collect()\n",
    "    # Get the intersection with the other days\n",
    "    if len(countries) < 1:\n",
    "        countries = set(countries500views)\n",
    "    else:\n",
    "        countries = countries.intersection(countries500views)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of the countries included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LT', 'MM', 'DZ', 'CI', 'TC', 'FI', 'AZ', 'SC', 'UA', 'RO', 'ZM', 'SL', 'NL', 'SB', 'LA', 'BS', 'MN', 'BW', 'PL', 'AM', 'PS', 'RE', 'MK', 'MX', 'PF', 'GL', 'EE', 'VG', 'SM', 'CN', 'AT', 'IQ', 'RU', 'NA', 'CG', 'AD', 'HR', 'SV', 'LI', 'CZ', 'NP', 'PT', 'SO', 'GG', 'PG', 'KY', 'GH', 'HK', 'CV', 'BN', 'LR', 'TW', 'BD', 'PY', 'LB', 'CL', 'TO', 'LY', 'ID', 'FK', 'SA', 'PK', 'AU', 'CA', 'BM', 'MW', 'BL', 'UZ', 'NE', 'GB', 'MT', 'YE', 'BR', 'KZ', 'BY', 'HN', 'NC', 'MD', 'GT', 'DE', 'AW', 'GN', 'ES', 'EC', 'IR', 'BH', 'MO', 'VI', 'IL', 'VE', 'TR', 'ME', 'MR', 'ZA', 'CR', 'SX', 'AI', 'GU', 'KR', 'TZ', 'US', 'RS', 'AL', 'MY', 'JM', 'IN', 'CK', 'LC', 'GM', 'AE', 'MQ', 'CM', 'RW', 'TG', 'FR', 'GF', 'CH', 'MG', 'TN', 'GQ', 'TL', 'GR', 'PA', 'GI', 'TD', 'SD', 'AG', 'DJ', 'MC', 'JO', 'BA', 'ET', 'SG', 'BF', 'CU', 'IT', 'GW', 'MV', 'FO', 'SE', 'BG', 'PH', 'FJ', 'WS', 'GE', 'SK', 'CW', 'MH', 'FM', 'LV', 'PE', 'MU', 'LS', 'GD', 'MZ', 'DM', 'DO', 'QA', 'XK', 'BZ', 'TH', 'EG', 'BI', 'BJ', 'MF', 'GY', 'JP', 'TM', 'VC', 'ZW', 'SN', 'NZ', 'OM', 'LK', 'BT', 'HU', 'KN', 'KE', 'SI', 'CY', 'ML', 'HT', 'GP', 'UG', 'IE', 'KW', 'VU', 'GA', 'BE', 'MA', 'AS', 'NI', 'KH', 'KG', 'TT', '--', 'NO', 'SY', 'BO', 'ER', 'CO', 'IM', 'UY', 'SS', 'NG', 'JE', 'YT', 'AR', 'CF', 'PW', 'PR', 'LU', 'SZ', 'VN', 'IS', 'MP', 'AF', 'BB', 'BQ', 'SR', 'DK', 'CD', 'TJ', 'AO', 'AX']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([c.country_code for c in countries500views])\n",
    "len(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|country_code|\n",
      "+------------+\n",
      "|          SE|\n",
      "|          CL|\n",
      "|          AZ|\n",
      "|          JE|\n",
      "|          SG|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_whitelist = spark.createDataFrame(sc.parallelize(list(countries))).cache()\n",
    "countries_whitelist.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the users with edits\n",
    "\n",
    "Get all the users indentifiers with an edit. Get the `user_identifier` and the day/month localised with the local timezone. This rows will left joined later as a blacklist to exclude all views of that user in the day of the edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total editors-day: 7063635\n",
      "Total editors: 6181225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[user_identifier: string, day: bigint, month: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select CONCAT(MD5(CONCAT(CONCAT(client_ip, '{}'), access_method)), MD5(CONCAT(user_agent, '{}'))) as user_identifier,\n",
    "        from_utc_timestamp(ts, geocoded_data['timezone']) as local_time\n",
    "from wmf.webrequest \n",
    "where day = {}\n",
    "AND month = {}\n",
    "AND year = {}\n",
    "AND agent_type = 'user'\n",
    "AND http_method= 'GET'\n",
    "AND ts is not NULL\n",
    "AND (uri_host = 'en.wikipedia.org' or uri_host = 'en.m.wikipedia.org')\n",
    "AND (uri_query LIKE '%action=edit%' OR uri_query LIKE '%action=visualeditor%' \n",
    "    OR uri_query LIKE '%&intestactions=edit&intestactionsdetail=full&uiprop=options%')\n",
    "\"\"\"\n",
    "\n",
    "edits_rdd = sc.emptyRDD()\n",
    "for day in daterange(start_date, end_date):\n",
    "    daily_edits_rdd = spark.sql(query.format(salt1, salt2, day.day, day.month, day.year))\\\n",
    "                .selectExpr(\"user_identifier\", \"day(local_time) day\", \"month(local_time) month\")\\\n",
    "                .distinct().repartition(30).rdd\n",
    "    edits_rdd = edits_rdd.union(daily_edits_rdd)\n",
    "    \n",
    "editors = spark.createDataFrame(edits_rdd).distinct().repartition(200).cache()\n",
    "\n",
    "print(\"Total editors-day:\", editors.count())\n",
    "print(\"Total editors:\", editors.select(\"user_identifier\").distinct().count())\n",
    "editors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "### Function to drop sensitive information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "editors.registerTempTable(\"editors\")\n",
    "\n",
    "def without_sensitive_data(current_dataframe):\n",
    "    # Add the day and month used in the join with the editors\n",
    "    current_dataframe = current_dataframe.selectExpr(\"*\", \"day(local_time) day\", \"month(local_time) month\")\n",
    "    current_dataframe.createOrReplaceTempView(\"current_dataframe\")\n",
    "    # LEFT JOIN on identifier and day/month. Keep non-matches to exclude editors\n",
    "    query_no_editors = \"\"\"\n",
    "        SELECT c.*\n",
    "        FROM current_dataframe c\n",
    "        LEFT JOIN editors e\n",
    "        ON c.user_identifier = e.user_identifier\n",
    "        AND c.day = e.day\n",
    "        AND c.month = e.month\n",
    "        WHERE e.user_identifier IS NULL\n",
    "    \"\"\"\n",
    "    clean_dataframe = spark.sql(query_no_editors).drop(\"day\").drop(\"month\")\n",
    "    # Join the dataframe with the whitelisted countries\n",
    "    return clean_dataframe.join(countries_whitelist, \"country_code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "# Get page views\n",
    "\n",
    "Let's start with the redirects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the redirect pages from the snapshot\n",
    "redirection_mapping = spark.sql(\"\"\"\n",
    "SELECT rd_from page_id, rd_title redirect_to\n",
    "FROM wmf_raw.mediawiki_redirect\n",
    "WHERE wiki_db = 'enwiki'\n",
    "AND snapshot = '{}'\n",
    "AND (rd_namespace = 0 OR rd_namespace = 4 OR rd_namespace = 100)\n",
    "\"\"\".format(SNAPSHOT))\n",
    "\n",
    "# Get the page title of the redirect\n",
    "# It is needed for the join with the pageloads\n",
    "redirection_mapping.createOrReplaceTempView(\"redirection_mapping\")\n",
    "title_to_title = spark.sql(\"\"\"\n",
    "SELECT mp.page_id page_id_from, mp.page_title page_title_from, rm.redirect_to page_title_to\n",
    "FROM wmf_raw.mediawiki_page mp\n",
    "JOIN redirection_mapping rm\n",
    "ON mp.page_id = rm.page_id\n",
    "WHERE mp.wiki_db = 'enwiki'\n",
    "AND mp.snapshot = '{}'\n",
    "AND (mp.page_namespace = 0 OR mp.page_namespace = 4 OR mp.page_namespace = 100)\n",
    "\"\"\".format(SNAPSHOT)).cache()\n",
    "\n",
    "title_to_title.createOrReplaceTempView(\"title_to_title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the pageviews. There is not need to anonymize the IP in this stage. We will drop it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-03 Country & editors filter keeps 97.66262710735279%\n",
      "2021-01-03 DONE\n",
      "2021-01-04 Country & editors filter keeps 97.23297041781927%\n",
      "2021-01-04 DONE\n",
      "2021-01-05 Country & editors filter keeps 96.65064891005593%\n",
      "2021-01-05 DONE\n",
      "2021-01-06 Country & editors filter keeps 97.08144648273644%\n",
      "2021-01-06 DONE\n",
      "2021-01-07 Country & editors filter keeps 96.9810369401506%\n",
      "2021-01-07 DONE\n",
      "2021-01-08 Country & editors filter keeps 97.1275888595591%\n",
      "2021-01-08 DONE\n",
      "2021-01-09 Country & editors filter keeps 97.66890696906167%\n",
      "2021-01-09 DONE\n",
      "2021-01-10 Country & editors filter keeps 97.50091142766524%\n",
      "2021-01-10 DONE\n",
      "2021-01-11 Country & editors filter keeps 96.81622148942999%\n",
      "2021-01-11 DONE\n",
      "2021-01-12 Country & editors filter keeps 96.84827740993283%\n",
      "2021-01-12 DONE\n",
      "2021-01-13 Country & editors filter keeps 97.11226632944532%\n",
      "2021-01-13 DONE\n",
      "2021-01-14 Country & editors filter keeps 96.85168497880122%\n",
      "2021-01-14 DONE\n",
      "2021-01-15 Country & editors filter keeps 96.9090164561678%\n",
      "2021-01-15 DONE\n",
      "2021-01-16 Country & editors filter keeps 97.47871952442475%\n",
      "2021-01-16 DONE\n",
      "2021-01-17 Country & editors filter keeps 97.20216756424502%\n",
      "2021-01-17 DONE\n",
      "2021-01-18 Country & editors filter keeps 97.23878467213527%\n",
      "2021-01-18 DONE\n",
      "2021-01-19 Country & editors filter keeps 96.96664375849608%\n",
      "2021-01-19 DONE\n",
      "2021-01-20 Country & editors filter keeps 96.81371942685396%\n",
      "2021-01-20 DONE\n",
      "2021-01-21 Country & editors filter keeps 96.72762707124099%\n",
      "2021-01-21 DONE\n",
      "2021-01-22 Country & editors filter keeps 96.80969761224675%\n",
      "2021-01-22 DONE\n",
      "2021-01-23 Country & editors filter keeps 97.45757509343083%\n",
      "2021-01-23 DONE\n",
      "2021-01-24 Country & editors filter keeps 97.53687539988113%\n",
      "2021-01-24 DONE\n",
      "2021-01-25 Country & editors filter keeps 97.01808483372096%\n",
      "2021-01-25 DONE\n",
      "2021-01-26 Country & editors filter keeps 97.17597099367627%\n",
      "2021-01-26 DONE\n",
      "2021-01-27 Country & editors filter keeps 97.15962877223626%\n",
      "2021-01-27 DONE\n",
      "2021-01-28 Country & editors filter keeps 97.13974710070516%\n",
      "2021-01-28 DONE\n",
      "2021-01-29 Country & editors filter keeps 97.20844705110302%\n",
      "2021-01-29 DONE\n",
      "2021-01-30 Country & editors filter keeps 97.75831794886541%\n",
      "2021-01-30 DONE\n",
      "2021-01-31 Country & editors filter keeps 97.8539198415088%\n",
      "2021-01-31 DONE\n",
      "Format:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[country_code: string, user_identifier: string, http_status: string, referer: string, local_time: timestamp, access_method: string, page_title: string, page_id: bigint, actual_destination: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# User must be not loggedIn, not a bot and the view must be in the English version of Wikipedia\n",
    "query = \"\"\"\n",
    "select CONCAT(MD5(CONCAT(CONCAT(client_ip, '{}'), access_method)), MD5(CONCAT(user_agent, '{}'))) as user_identifier,\n",
    "        geocoded_data['country_code'] country_code,\n",
    "        http_status, referer,\n",
    "        from_utc_timestamp(ts, geocoded_data['timezone']) as local_time,\n",
    "        access_method,\n",
    "        pageview_info['page_title'] as page_title, page_id\n",
    "from wmf.webrequest \n",
    "where day = {}\n",
    "AND month = {}\n",
    "AND year = {}\n",
    "AND x_analytics_map['loggedIn'] is NULL\n",
    "AND (namespace_id = 0 OR namespace_id = 4 OR namespace_id = 100)\n",
    "AND agent_type = 'user'\n",
    "AND is_pageview = TRUE\n",
    "AND (uri_host = 'en.wikipedia.org' or uri_host = 'en.m.wikipedia.org')\n",
    "AND http_method= 'GET'\n",
    "AND ts is not NULL\n",
    "\"\"\"\n",
    "\n",
    "# Repeat for each day\n",
    "for day in daterange(start_date, end_date):\n",
    "    pageview_logs = spark.sql(query.format(salt1, salt2, day.day, day.month, day.year)).repartition(30)\n",
    "    pageview_logs.createOrReplaceTempView(\"pageview_logs\")\n",
    "    \n",
    "    # Add resolved redirects (add the day and month for later join)\n",
    "    query_join_redirects = \"\"\"\n",
    "        SELECT l.*, \n",
    "            \n",
    "            CASE WHEN tt.page_title_to is NULL \n",
    "                THEN l.page_title \n",
    "                ELSE tt.page_title_to \n",
    "            END as actual_destination\n",
    "        FROM pageview_logs l\n",
    "        LEFT JOIN title_to_title tt\n",
    "        ON l.page_title = tt.page_title_from\n",
    "    \"\"\"\n",
    "    logs_redirect_resolved = spark.sql(query_join_redirects).cache()\n",
    "    \n",
    "    # Remove sensitive data\n",
    "    logs_filtered = without_sensitive_data(logs_redirect_resolved).cache()\n",
    "\n",
    "    print(\"{}-{:02d}-{:02d} Country & editors filter keeps {}%\".format(day.year, day.month, day.day, \n",
    "                                     100*logs_filtered.count() / logs_redirect_resolved.count()))\n",
    "    # Write the pageviews of this day\n",
    "    logs_filtered.write.mode(\"overwrite\")\\\n",
    "                .parquet(\"{}/pageloads/{}{:02d}{:02d}\".format(DESTINATION_FOLDER, day.year, day.month, day.day))\n",
    "    print(\"{}-{:02d}-{:02d} DONE\".format(day.year, day.month, day.day))\n",
    "    \n",
    "    # Drop cached dataframes\n",
    "    logs_redirect_resolved.unpersist()\n",
    "    logs_filtered.unpersist()\n",
    "    \n",
    "print(\"Format:\")\n",
    "logs_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get page previews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-03 DONE\n",
      "2021-01-04 DONE\n",
      "2021-01-05 DONE\n",
      "2021-01-06 DONE\n",
      "2021-01-07 DONE\n",
      "2021-01-08 DONE\n",
      "2021-01-09 DONE\n",
      "2021-01-10 DONE\n",
      "2021-01-11 DONE\n",
      "2021-01-12 DONE\n",
      "2021-01-13 DONE\n",
      "2021-01-14 DONE\n",
      "2021-01-15 DONE\n",
      "2021-01-16 DONE\n",
      "2021-01-17 DONE\n",
      "2021-01-18 DONE\n",
      "2021-01-19 DONE\n",
      "2021-01-20 DONE\n",
      "2021-01-21 DONE\n",
      "2021-01-22 DONE\n",
      "2021-01-23 DONE\n",
      "2021-01-24 DONE\n",
      "2021-01-25 DONE\n",
      "2021-01-26 DONE\n",
      "2021-01-27 DONE\n",
      "2021-01-28 DONE\n",
      "2021-01-29 DONE\n",
      "2021-01-30 DONE\n",
      "2021-01-31 DONE\n",
      "Format:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[user_identifier: string, http_status: string, local_time: timestamp, preview_title: string, page_title: string, long_preview: boolean]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_page_udf = udf(lambda uri_path: unquote(uri_path[uri_path.rfind('/')+1:]), StringType())\n",
    "\n",
    "# Consider all previews. Note:\n",
    "#    year year_original, month month_original, day day_original, \n",
    "#    hour hour_original, client_ip, user_agent_map: are required for the join with the long previews\n",
    "# We'll discard them later.\n",
    "preview_query = \"\"\"\n",
    "select CONCAT(MD5(CONCAT(CONCAT(client_ip, '{}'), access_method)), MD5(CONCAT(user_agent, '{}'))) as user_identifier,\n",
    "        http_status, referer,\n",
    "        from_utc_timestamp(ts, geocoded_data['timezone']) as local_time,\n",
    "        uri_path,\n",
    "        geocoded_data['country_code'] country_code,\n",
    "        \n",
    "        year year_original, month month_original, day day_original, hour hour_original, client_ip, user_agent_map\n",
    "from wmf.webrequest \n",
    "where day = {}\n",
    "AND month = {}\n",
    "AND year = {}\n",
    "AND x_analytics_map['loggedIn'] is NULL\n",
    "AND agent_type = 'user'\n",
    "AND http_method= 'GET'\n",
    "AND ts is not NULL\n",
    "AND (uri_host = 'en.wikipedia.org' or uri_host = 'en.m.wikipedia.org')\n",
    "AND uri_path LIKE '/api/rest_v1/page/summary/%'\n",
    "AND referer LIKE '%wikipedia.org/wiki%'\n",
    "\"\"\"\n",
    "\n",
    "# Consider the previews with at least 1 second\n",
    "long_previews_query = \"\"\"\n",
    "select ip, \n",
    "    year, month, day, hour,\n",
    "    useragent.browser_family,\n",
    "    useragent.browser_major,\n",
    "    useragent.os_family,\n",
    "    useragent.device_family,\n",
    "    event.source_title, event.page_title preview_title\n",
    "from event.virtualpageview\n",
    "where year = {}\n",
    "and month = {}\n",
    "and day = {}\n",
    "and webhost = 'en.wikipedia.org'\n",
    "and useragent.is_bot = false\n",
    "\"\"\"\n",
    "\n",
    "for day in daterange(start_date, end_date):\n",
    "    # compile the query with parameters and repartition the data\n",
    "    logs = spark.sql(preview_query.format(salt1, salt2, day.day, day.month, day.year)).repartition(30)\n",
    "    \n",
    "    # Extract the titles from the urls and drop the original fields\n",
    "    logs_cleaned = logs.withColumn(\"preview_title\", get_page_udf(logs.uri_path)).drop(\"uri_path\")\\\n",
    "                    .withColumn(\"page_title\", get_page_udf(logs.referer)).drop(\"referer\")\n",
    "    \n",
    "    # Remove sensitive data\n",
    "    previews_cleaned = without_sensitive_data(logs_cleaned).drop(\"country_code\")\n",
    "    \n",
    "    # Get long previews\n",
    "    long_previews = spark.sql(long_previews_query.format(day.year, day.month, day.day)).distinct()\n",
    "\n",
    "    # Register the tables\n",
    "    previews_cleaned.createOrReplaceTempView(\"previews_cleaned\")\n",
    "    long_previews.createOrReplaceTempView(\"long_previews\")\n",
    "\n",
    "    # LEFT JOIN previews_cleaned with the long previews to add a label TRUE or FALSE\n",
    "    # Since the 2 tables have different schema, we have to maximise the matching by joining\n",
    "    # on all the fields we can use. We add a boolean labels to mark if we found a long preview\n",
    "    # from the same ip, on the same page, for the same link, in the the same hour, \n",
    "    # with the same browser/OS version\n",
    "    query = \"\"\"\n",
    "        SELECT ap.*,\n",
    "            CASE \n",
    "                WHEN lp.ip IS NULL THEN FALSE\n",
    "                ELSE TRUE\n",
    "            END as long_preview\n",
    "        FROM previews_cleaned ap\n",
    "        LEFT JOIN long_previews lp\n",
    "        ON ap.client_ip = lp.ip\n",
    "        AND ap.hour_original = lp.hour\n",
    "        AND ap.day_original = lp.day\n",
    "        AND ap.month_original = lp.month\n",
    "        AND ap.year_original = lp.year\n",
    "        AND ap.page_title = lp.source_title\n",
    "        AND ap.preview_title = lp.preview_title\n",
    "        AND ap.user_agent_map.browser_family = lp.browser_family\n",
    "        AND ap.user_agent_map.browser_major = lp.browser_major\n",
    "        AND ap.user_agent_map.os_family = lp.os_family\n",
    "        AND ap.user_agent_map.device_family = lp.device_family\n",
    "    \"\"\"\n",
    "\n",
    "    all_previews_labeled = spark.sql(query).drop(\"user_agent_map\", \"client_ip\", \"year_original\", \n",
    "                                                 \"month_original\", \"day_original\", \"hour_original\")\n",
    "    \n",
    "    # Write the previews\n",
    "    all_previews_labeled.write.mode(\"overwrite\")\\\n",
    "                .parquet(\"{}/previews/{}{:02d}{:02d}\".format(DESTINATION_FOLDER, day.year, day.month, day.day))\n",
    "    print(\"{}-{:02d}-{:02d} DONE\".format(day.year, day.month, day.day))\n",
    "    \n",
    "print(\"Format:\")\n",
    "all_previews_labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Get clicks on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a row with the image name and title extracted from the urls\n",
    "# Input format:\n",
    "# DataFrame[user_identifier: string, local_time: timestamp, \n",
    "#           referer: string, uri_query: string, access_method: string]\n",
    "\n",
    "def get_formatted_row(row):\n",
    "    try:\n",
    "        # Take the string after the last / and convert the http encoded text\n",
    "        title = unquote(row.referer[row.referer.rfind('/')+1:])\n",
    "        # Parse the query string\n",
    "        parsed_query = parse_qs(unquote(row.uri_query))\n",
    "        # Desktop and mobile have different formats\n",
    "        if row.access_method == 'desktop':\n",
    "            # Get the parameters called uri\n",
    "            image_url = parsed_query.get(\"uri\")\n",
    "            if len(image_url)>0:\n",
    "                image_name = unquote(image_url[0][image_url[0].rfind('/')+1:])\n",
    "        else:\n",
    "            # Get the titles parameter and then the string after File:\n",
    "            file_name = parsed_query.get('titles')[0]\n",
    "            image_title = file_name.split('File:')[1]\n",
    "            image_name = unquote(image_title)\n",
    "    except:\n",
    "        # If there is no image return null\n",
    "        image_name = None\n",
    "    \n",
    "    return Row(user_identifier=row.user_identifier, access_method=row.access_method, local_time=row.local_time,\n",
    "                       title=title, image_name=image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-03 DONE\n",
      "2021-01-04 DONE\n",
      "2021-01-05 DONE\n",
      "2021-01-06 DONE\n",
      "2021-01-07 DONE\n",
      "2021-01-08 DONE\n",
      "2021-01-09 DONE\n",
      "2021-01-10 DONE\n",
      "2021-01-11 DONE\n",
      "2021-01-12 DONE\n",
      "2021-01-13 DONE\n",
      "2021-01-14 DONE\n",
      "2021-01-15 DONE\n",
      "2021-01-16 DONE\n",
      "2021-01-17 DONE\n",
      "2021-01-18 DONE\n",
      "2021-01-19 DONE\n",
      "2021-01-20 DONE\n",
      "2021-01-21 DONE\n",
      "2021-01-22 DONE\n",
      "2021-01-23 DONE\n",
      "2021-01-24 DONE\n",
      "2021-01-25 DONE\n",
      "2021-01-26 DONE\n",
      "2021-01-27 DONE\n",
      "2021-01-28 DONE\n",
      "2021-01-29 DONE\n",
      "2021-01-30 DONE\n",
      "2021-01-31 DONE\n",
      "Format:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[access_method: string, image_name: string, local_time: timestamp, title: string, user_identifier: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The referer filter limits to image previews opened from Wikipedia articles\n",
    "imageviews_query = \"\"\"\n",
    "    select CONCAT(MD5(CONCAT(CONCAT(client_ip, '{}'), access_method)), MD5(CONCAT(user_agent, '{}'))) as user_identifier,\n",
    "        from_utc_timestamp(ts, geocoded_data['timezone']) as local_time,\n",
    "        referer, uri_query, access_method,\n",
    "        geocoded_data['country_code'] country_code\n",
    "    from wmf.webrequest \n",
    "    where day = {}\n",
    "    AND month = {}\n",
    "    AND year = {}\n",
    "    AND x_analytics_map['loggedIn'] is NULL\n",
    "    AND agent_type = 'user'\n",
    "    AND ts is not NULL\n",
    "    AND (uri_host = 'en.wikipedia.org' or uri_host = 'en.m.wikipedia.org')\n",
    "    AND referer LIKE '%wikipedia.org%'\n",
    "    AND (\n",
    "           (uri_path LIKE '%/beacon/media%' AND access_method = 'desktop')\n",
    "        OR (uri_path = '/w/api.php' AND uri_query LIKE '%prop=imageinfo%' AND access_method = 'mobile web')\n",
    "        )\n",
    "\"\"\"\n",
    "\n",
    "for day in daterange(start_date, end_date):\n",
    "    imageviews = spark.sql(imageviews_query.format(salt1, salt2, day.day, day.month, day.year)).repartition(30)\n",
    "    \n",
    "    # Remove sensitive data\n",
    "    imageviews_cleaned = without_sensitive_data(imageviews).drop(\"country_code\")\n",
    "    \n",
    "    # Extract the titles from the urls and drop the original fields\n",
    "    imageviews_with_titles = spark.createDataFrame(imageviews_cleaned.rdd.map(get_formatted_row))\n",
    "    imageviews_with_titles.write.mode(\"overwrite\")\\\n",
    "                .parquet(\"{}/imageviews/{}{:02d}{:02d}\".format(DESTINATION_FOLDER, day.year, day.month, day.day))\n",
    "    print(\"{}-{:02d}-{:02d} DONE\".format(day.year, day.month, day.day))\n",
    "    \n",
    "print(\"Format:\")\n",
    "imageviews_with_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark - YARN (large)",
   "language": "python",
   "name": "spark_yarn_pyspark_large"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
